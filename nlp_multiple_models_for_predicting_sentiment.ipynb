{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l10c02_nlp_multiple_models_for_predicting_sentiment.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit"
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAclqSm3OOml"
      },
      "source": [
        "# Using LSTMs, CNNs, GRUs with a larger dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fentd-GnIj-j"
      },
      "source": [
        "In this colab, you use different kinds of layers to see how they affect the model.\n",
        "\n",
        "You will use the glue/sst2 dataset, which is available through tensorflow_datasets. \n",
        "\n",
        "The General Language Understanding Evaluation (GLUE) benchmark (https://gluebenchmark.com/) is a collection of resources for training, evaluating, and analyzing natural language understanding systems.\n",
        "\n",
        "These resources include the Stanford Sentiment Treebank (SST) dataset that consists of sentences from movie reviews and human annotations of their sentiment. This colab uses version 2 of the SST dataset.\n",
        "\n",
        "The splits are:\n",
        "\n",
        "*   train\t67,349\n",
        "*   validation\t872\n",
        "\n",
        "\n",
        "and the column headings are:\n",
        "\n",
        "*   sentence\n",
        "*   label\n",
        "\n",
        "\n",
        "For more information about the dataset, see [https://www.tensorflow.org/datasets/catalog/glue#gluesst2](https://www.tensorflow.org/datasets/catalog/glue#gluesst2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L62G7LTwNzoD"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2P6xdtIJMyc"
      },
      "source": [
        "# Get the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCOtiRJZbxCH"
      },
      "source": [
        "# Get the dataset.\n",
        "# It has 70000 items, so might take a while to download\n",
        "dataset, info = tfds.load('glue/sst2', with_info=True)\n",
        "print(info.features)\n",
        "print(info.features[\"label\"].num_classes)\n",
        "print(info.features[\"label\"].names)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset 7.09 MiB (download: 7.09 MiB, generated: Unknown size, total: 7.09 MiB) to /home/ritesh/tensorflow_datasets/glue/sst2/1.0.0...\u001b[0m\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\n",
            "Dl Size...:   0%|          | 0/7 [00:01<?, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
            "Dl Size...:  14%|█▍        | 1/7 [00:03<00:20,  3.33s/ MiB]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
            "Dl Size...:  29%|██▊       | 2/7 [00:03<00:08,  1.62s/ MiB]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\n",
            "Dl Size...:  43%|████▎     | 3/7 [00:03<00:03,  1.04 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
            "Dl Size...:  57%|█████▋    | 4/7 [00:04<00:02,  1.46 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
            "Dl Size...:  71%|███████▏  | 5/7 [00:04<00:01,  1.83 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\n",
            "Dl Size...:  86%|████████▌ | 6/7 [00:04<00:00,  2.16 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:04, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\n",
            "Dl Size...: 100%|██████████| 7/7 [00:05<00:00,  2.53 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.48s/ url]\n",
            "Dl Size...: 100%|██████████| 7/7 [00:05<00:00,  2.53 MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.48s/ url]\n",
            "Dl Size...: 100%|██████████| 7/7 [00:05<00:00,  2.53 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...:   0%|          | 0/1 [00:05<?, ? file/s]\u001b[A\u001b[A\n",
            "\n",
            "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.48s/ url]\n",
            "Dl Size...: 100%|██████████| 7/7 [00:05<00:00,  2.53 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 100%|██████████| 1/1 [00:05<00:00,  5.76s/ file]\n",
            "Dl Size...: 100%|██████████| 7/7 [00:05<00:00,  1.21 MiB/s]\n",
            "Dl Completed...: 100%|██████████| 1/1 [00:05<00:00,  5.77s/ url]\n",
            "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]\n",
            "Generating train examples...:   0%|          | 0/67349 [00:00<?, ? examples/s]\u001b[A\n",
            "Generating train examples...:   1%|          | 376/67349 [00:00<00:17, 3756.59 examples/s]\u001b[A\n",
            "Generating train examples...:   1%|          | 752/67349 [00:00<00:17, 3754.11 examples/s]\u001b[A\n",
            "Generating train examples...:   2%|▏         | 1133/67349 [00:00<00:17, 3770.72 examples/s]\u001b[A\n",
            "Generating train examples...:   2%|▏         | 1625/67349 [00:00<00:15, 4221.54 examples/s]\u001b[A\n",
            "Generating train examples...:   3%|▎         | 2133/67349 [00:00<00:14, 4529.41 examples/s]\u001b[A\n",
            "Generating train examples...:   4%|▍         | 2587/67349 [00:00<00:15, 4215.33 examples/s]\u001b[A\n",
            "Generating train examples...:   4%|▍         | 3013/67349 [00:00<00:16, 3860.48 examples/s]\u001b[A\n",
            "Generating train examples...:   5%|▌         | 3406/67349 [00:00<00:16, 3793.55 examples/s]\u001b[A\n",
            "Generating train examples...:   6%|▌         | 3923/67349 [00:00<00:15, 4188.70 examples/s]\u001b[A\n",
            "Generating train examples...:   6%|▋         | 4367/67349 [00:01<00:14, 4260.97 examples/s]\u001b[A\n",
            "Generating train examples...:   7%|▋         | 4800/67349 [00:01<00:14, 4280.91 examples/s]\u001b[A\n",
            "Generating train examples...:   8%|▊         | 5232/67349 [00:01<00:14, 4283.57 examples/s]\u001b[A\n",
            "Generating train examples...:   8%|▊         | 5663/67349 [00:01<00:15, 4013.80 examples/s]\u001b[A\n",
            "Generating train examples...:   9%|▉         | 6070/67349 [00:01<00:15, 4002.68 examples/s]\u001b[A\n",
            "Generating train examples...:  10%|▉         | 6474/67349 [00:01<00:15, 4002.79 examples/s]\u001b[A\n",
            "Generating train examples...:  10%|█         | 6963/67349 [00:01<00:14, 4258.57 examples/s]\u001b[A\n",
            "Generating train examples...:  11%|█         | 7481/67349 [00:01<00:13, 4526.67 examples/s]\u001b[A\n",
            "Generating train examples...:  12%|█▏        | 7983/67349 [00:01<00:12, 4670.12 examples/s]\u001b[A\n",
            "Generating train examples...:  13%|█▎        | 8452/67349 [00:01<00:12, 4550.10 examples/s]\u001b[A\n",
            "Generating train examples...:  13%|█▎        | 8914/67349 [00:02<00:12, 4569.67 examples/s]\u001b[A\n",
            "Generating train examples...:  14%|█▍        | 9373/67349 [00:02<00:12, 4484.82 examples/s]\u001b[A\n",
            "Generating train examples...:  15%|█▍        | 9823/67349 [00:02<00:13, 4373.64 examples/s]\u001b[A\n",
            "Generating train examples...:  15%|█▌        | 10304/67349 [00:02<00:12, 4497.82 examples/s]\u001b[A\n",
            "Generating train examples...:  16%|█▌        | 10807/67349 [00:02<00:12, 4650.98 examples/s]\u001b[A\n",
            "Generating train examples...:  17%|█▋        | 11274/67349 [00:02<00:12, 4633.76 examples/s]\u001b[A\n",
            "Generating train examples...:  18%|█▊        | 11809/67349 [00:02<00:11, 4843.18 examples/s]\u001b[A\n",
            "Generating train examples...:  18%|█▊        | 12334/67349 [00:02<00:11, 4962.21 examples/s]\u001b[A\n",
            "Generating train examples...:  19%|█▉        | 12832/67349 [00:02<00:11, 4912.41 examples/s]\u001b[A\n",
            "Generating train examples...:  20%|█▉        | 13339/67349 [00:03<00:10, 4957.46 examples/s]\u001b[A\n",
            "Generating train examples...:  21%|██        | 13852/67349 [00:03<00:10, 5008.17 examples/s]\u001b[A\n",
            "Generating train examples...:  21%|██▏       | 14383/67349 [00:03<00:10, 5097.05 examples/s]\u001b[A\n",
            "Generating train examples...:  22%|██▏       | 14894/67349 [00:03<00:10, 5096.10 examples/s]\u001b[A\n",
            "Generating train examples...:  23%|██▎       | 15413/67349 [00:03<00:10, 5122.07 examples/s]\u001b[A\n",
            "Generating train examples...:  24%|██▎       | 15930/67349 [00:03<00:10, 5134.34 examples/s]\u001b[A\n",
            "Generating train examples...:  24%|██▍       | 16444/67349 [00:03<00:10, 5007.10 examples/s]\u001b[A\n",
            "Generating train examples...:  25%|██▌       | 16955/67349 [00:03<00:10, 5036.52 examples/s]\u001b[A\n",
            "Generating train examples...:  26%|██▌       | 17460/67349 [00:03<00:10, 4709.03 examples/s]\u001b[A\n",
            "Generating train examples...:  27%|██▋       | 17958/67349 [00:03<00:10, 4784.55 examples/s]\u001b[A\n",
            "Generating train examples...:  27%|██▋       | 18441/67349 [00:04<00:10, 4794.82 examples/s]\u001b[A\n",
            "Generating train examples...:  28%|██▊       | 18924/67349 [00:04<00:10, 4686.43 examples/s]\u001b[A\n",
            "Generating train examples...:  29%|██▉       | 19395/67349 [00:04<00:10, 4645.08 examples/s]\u001b[A\n",
            "Generating train examples...:  29%|██▉       | 19864/67349 [00:04<00:10, 4657.22 examples/s]\u001b[A\n",
            "Generating train examples...:  30%|███       | 20331/67349 [00:04<00:10, 4648.42 examples/s]\u001b[A\n",
            "Generating train examples...:  31%|███       | 20797/67349 [00:04<00:10, 4636.05 examples/s]\u001b[A\n",
            "Generating train examples...:  32%|███▏      | 21274/67349 [00:04<00:09, 4672.52 examples/s]\u001b[A\n",
            "Generating train examples...:  32%|███▏      | 21771/67349 [00:04<00:09, 4756.12 examples/s]\u001b[A\n",
            "Generating train examples...:  33%|███▎      | 22282/67349 [00:04<00:09, 4859.10 examples/s]\u001b[A\n",
            "Generating train examples...:  34%|███▍      | 22769/67349 [00:04<00:09, 4666.82 examples/s]\u001b[A\n",
            "Generating train examples...:  35%|███▍      | 23267/67349 [00:05<00:09, 4755.30 examples/s]\u001b[A\n",
            "Generating train examples...:  35%|███▌      | 23767/67349 [00:05<00:09, 4824.76 examples/s]\u001b[A\n",
            "Generating train examples...:  36%|███▌      | 24274/67349 [00:05<00:08, 4896.57 examples/s]\u001b[A\n",
            "Generating train examples...:  37%|███▋      | 24783/67349 [00:05<00:08, 4951.59 examples/s]\u001b[A\n",
            "Generating train examples...:  38%|███▊      | 25291/67349 [00:05<00:08, 4989.62 examples/s]\u001b[A\n",
            "Generating train examples...:  38%|███▊      | 25801/67349 [00:05<00:08, 5020.38 examples/s]\u001b[A\n",
            "Generating train examples...:  39%|███▉      | 26304/67349 [00:05<00:08, 4988.69 examples/s]\u001b[A\n",
            "Generating train examples...:  40%|███▉      | 26818/67349 [00:05<00:08, 5033.18 examples/s]\u001b[A\n",
            "Generating train examples...:  41%|████      | 27322/67349 [00:05<00:07, 5028.61 examples/s]\u001b[A\n",
            "Generating train examples...:  41%|████▏     | 27833/67349 [00:05<00:07, 5051.12 examples/s]\u001b[A\n",
            "Generating train examples...:  42%|████▏     | 28339/67349 [00:06<00:07, 5022.78 examples/s]\u001b[A\n",
            "Generating train examples...:  43%|████▎     | 28859/67349 [00:06<00:07, 5075.19 examples/s]\u001b[A\n",
            "Generating train examples...:  44%|████▎     | 29377/67349 [00:06<00:07, 5104.19 examples/s]\u001b[A\n",
            "Generating train examples...:  44%|████▍     | 29888/67349 [00:06<00:07, 5050.76 examples/s]\u001b[A\n",
            "Generating train examples...:  45%|████▌     | 30394/67349 [00:06<00:07, 4971.69 examples/s]\u001b[A\n",
            "Generating train examples...:  46%|████▌     | 30892/67349 [00:06<00:07, 4942.52 examples/s]\u001b[A\n",
            "Generating train examples...:  47%|████▋     | 31387/67349 [00:06<00:07, 4677.26 examples/s]\u001b[A\n",
            "Generating train examples...:  47%|████▋     | 31858/67349 [00:06<00:07, 4525.62 examples/s]\u001b[A\n",
            "Generating train examples...:  48%|████▊     | 32313/67349 [00:06<00:08, 4340.05 examples/s]\u001b[A\n",
            "Generating train examples...:  49%|████▊     | 32750/67349 [00:07<00:08, 4296.62 examples/s]\u001b[A\n",
            "Generating train examples...:  49%|████▉     | 33182/67349 [00:07<00:08, 4254.88 examples/s]\u001b[A\n",
            "Generating train examples...:  50%|████▉     | 33609/67349 [00:07<00:08, 4184.74 examples/s]\u001b[A\n",
            "Generating train examples...:  51%|█████     | 34051/67349 [00:07<00:07, 4250.95 examples/s]\u001b[A\n",
            "Generating train examples...:  51%|█████▏    | 34557/67349 [00:07<00:07, 4484.91 examples/s]\u001b[A\n",
            "Generating train examples...:  52%|█████▏    | 35020/67349 [00:07<00:07, 4524.27 examples/s]\u001b[A\n",
            "Generating train examples...:  53%|█████▎    | 35474/67349 [00:07<00:07, 4389.98 examples/s]\u001b[A\n",
            "Generating train examples...:  53%|█████▎    | 35917/67349 [00:07<00:07, 4399.28 examples/s]\u001b[A\n",
            "Generating train examples...:  54%|█████▍    | 36405/67349 [00:07<00:06, 4538.58 examples/s]\u001b[A\n",
            "Generating train examples...:  55%|█████▍    | 36860/67349 [00:07<00:07, 4298.69 examples/s]\u001b[A\n",
            "Generating train examples...:  55%|█████▌    | 37293/67349 [00:08<00:07, 4088.56 examples/s]\u001b[A\n",
            "Generating train examples...:  56%|█████▌    | 37787/67349 [00:08<00:06, 4325.44 examples/s]\u001b[A\n",
            "Generating train examples...:  57%|█████▋    | 38295/67349 [00:08<00:06, 4540.26 examples/s]\u001b[A\n",
            "Generating train examples...:  58%|█████▊    | 38754/67349 [00:08<00:06, 4101.28 examples/s]\u001b[A\n",
            "Generating train examples...:  58%|█████▊    | 39175/67349 [00:08<00:07, 3993.55 examples/s]\u001b[A\n",
            "Generating train examples...:  59%|█████▉    | 39582/67349 [00:08<00:06, 3973.55 examples/s]\u001b[A\n",
            "Generating train examples...:  59%|█████▉    | 39985/67349 [00:08<00:06, 3965.37 examples/s]\u001b[A\n",
            "Generating train examples...:  60%|█████▉    | 40385/67349 [00:08<00:07, 3621.95 examples/s]\u001b[A\n",
            "Generating train examples...:  61%|██████    | 40811/67349 [00:09<00:06, 3793.23 examples/s]\u001b[A\n",
            "Generating train examples...:  61%|██████    | 41198/67349 [00:09<00:06, 3737.56 examples/s]\u001b[A\n",
            "Generating train examples...:  62%|██████▏   | 41702/67349 [00:09<00:06, 4101.03 examples/s]\u001b[A\n",
            "Generating train examples...:  63%|██████▎   | 42192/67349 [00:09<00:05, 4327.36 examples/s]\u001b[A\n",
            "Generating train examples...:  63%|██████▎   | 42630/67349 [00:09<00:05, 4272.24 examples/s]\u001b[A\n",
            "Generating train examples...:  64%|██████▍   | 43061/67349 [00:09<00:05, 4057.89 examples/s]\u001b[A\n",
            "Generating train examples...:  65%|██████▍   | 43472/67349 [00:09<00:05, 3995.51 examples/s]\u001b[A\n",
            "Generating train examples...:  65%|██████▌   | 43875/67349 [00:09<00:05, 3920.85 examples/s]\u001b[A\n",
            "Generating train examples...:  66%|██████▌   | 44329/67349 [00:09<00:05, 4092.67 examples/s]\u001b[A\n",
            "Generating train examples...:  66%|██████▋   | 44782/67349 [00:09<00:05, 4217.40 examples/s]\u001b[A\n",
            "Generating train examples...:  67%|██████▋   | 45280/67349 [00:10<00:04, 4438.34 examples/s]\u001b[A\n",
            "Generating train examples...:  68%|██████▊   | 45797/67349 [00:10<00:04, 4651.65 examples/s]\u001b[A\n",
            "Generating train examples...:  69%|██████▊   | 46278/67349 [00:10<00:04, 4696.28 examples/s]\u001b[A\n",
            "Generating train examples...:  69%|██████▉   | 46750/67349 [00:10<00:04, 4674.75 examples/s]\u001b[A\n",
            "Generating train examples...:  70%|███████   | 47219/67349 [00:10<00:04, 4474.39 examples/s]\u001b[A\n",
            "Generating train examples...:  71%|███████   | 47669/67349 [00:10<00:04, 4411.42 examples/s]\u001b[A\n",
            "Generating train examples...:  71%|███████▏  | 48112/67349 [00:10<00:04, 4376.61 examples/s]\u001b[A\n",
            "Generating train examples...:  72%|███████▏  | 48551/67349 [00:10<00:04, 4325.61 examples/s]\u001b[A\n",
            "Generating train examples...:  73%|███████▎  | 49034/67349 [00:10<00:04, 4470.35 examples/s]\u001b[A\n",
            "Generating train examples...:  73%|███████▎  | 49483/67349 [00:10<00:04, 4402.50 examples/s]\u001b[A\n",
            "Generating train examples...:  74%|███████▍  | 49952/67349 [00:11<00:03, 4483.97 examples/s]\u001b[A\n",
            "Generating train examples...:  75%|███████▍  | 50457/67349 [00:11<00:03, 4648.65 examples/s]\u001b[A\n",
            "Generating train examples...:  76%|███████▌  | 50928/67349 [00:11<00:03, 4665.79 examples/s]\u001b[A\n",
            "Generating train examples...:  76%|███████▋  | 51429/67349 [00:11<00:03, 4766.27 examples/s]\u001b[A\n",
            "Generating train examples...:  77%|███████▋  | 51924/67349 [00:11<00:03, 4819.63 examples/s]\u001b[A\n",
            "Generating train examples...:  78%|███████▊  | 52407/67349 [00:11<00:03, 4732.55 examples/s]\u001b[A\n",
            "Generating train examples...:  79%|███████▊  | 52901/67349 [00:11<00:03, 4791.41 examples/s]\u001b[A\n",
            "Generating train examples...:  79%|███████▉  | 53381/67349 [00:11<00:02, 4750.32 examples/s]\u001b[A\n",
            "Generating train examples...:  80%|███████▉  | 53858/67349 [00:11<00:02, 4754.51 examples/s]\u001b[A\n",
            "Generating train examples...:  81%|████████  | 54334/67349 [00:12<00:02, 4676.73 examples/s]\u001b[A\n",
            "Generating train examples...:  81%|████████▏ | 54809/67349 [00:12<00:02, 4696.56 examples/s]\u001b[A\n",
            "Generating train examples...:  82%|████████▏ | 55296/67349 [00:12<00:02, 4745.91 examples/s]\u001b[A\n",
            "Generating train examples...:  83%|████████▎ | 55771/67349 [00:12<00:02, 4744.84 examples/s]\u001b[A\n",
            "Generating train examples...:  84%|████████▎ | 56246/67349 [00:12<00:02, 4730.29 examples/s]\u001b[A\n",
            "Generating train examples...:  84%|████████▍ | 56720/67349 [00:12<00:02, 4482.65 examples/s]\u001b[A\n",
            "Generating train examples...:  85%|████████▍ | 57171/67349 [00:12<00:02, 4465.96 examples/s]\u001b[A\n",
            "Generating train examples...:  86%|████████▌ | 57621/67349 [00:12<00:02, 4475.57 examples/s]\u001b[A\n",
            "Generating train examples...:  86%|████████▌ | 58085/67349 [00:12<00:02, 4522.33 examples/s]\u001b[A\n",
            "Generating train examples...:  87%|████████▋ | 58539/67349 [00:12<00:01, 4411.67 examples/s]\u001b[A\n",
            "Generating train examples...:  88%|████████▊ | 58992/67349 [00:13<00:01, 4445.61 examples/s]\u001b[A\n",
            "Generating train examples...:  88%|████████▊ | 59452/67349 [00:13<00:01, 4488.15 examples/s]\u001b[A\n",
            "Generating train examples...:  89%|████████▉ | 59902/67349 [00:13<00:01, 4429.20 examples/s]\u001b[A\n",
            "Generating train examples...:  90%|████████▉ | 60346/67349 [00:13<00:01, 4404.65 examples/s]\u001b[A\n",
            "Generating train examples...:  90%|█████████ | 60801/67349 [00:13<00:01, 4447.01 examples/s]\u001b[A\n",
            "Generating train examples...:  91%|█████████ | 61278/67349 [00:13<00:01, 4540.22 examples/s]\u001b[A\n",
            "Generating train examples...:  92%|█████████▏| 61733/67349 [00:13<00:01, 4528.58 examples/s]\u001b[A\n",
            "Generating train examples...:  92%|█████████▏| 62187/67349 [00:13<00:01, 4526.43 examples/s]\u001b[A\n",
            "Generating train examples...:  93%|█████████▎| 62640/67349 [00:13<00:01, 4310.67 examples/s]\u001b[A\n",
            "Generating train examples...:  94%|█████████▎| 63074/67349 [00:13<00:00, 4304.74 examples/s]\u001b[A\n",
            "Generating train examples...:  94%|█████████▍| 63506/67349 [00:14<00:00, 4078.63 examples/s]\u001b[A\n",
            "Generating train examples...:  95%|█████████▍| 63917/67349 [00:14<00:00, 4024.55 examples/s]\u001b[A\n",
            "Generating train examples...:  96%|█████████▌| 64394/67349 [00:14<00:00, 4235.95 examples/s]\u001b[A\n",
            "Generating train examples...:  96%|█████████▌| 64821/67349 [00:14<00:00, 4094.20 examples/s]\u001b[A\n",
            "Generating train examples...:  97%|█████████▋| 65314/67349 [00:14<00:00, 4331.81 examples/s]\u001b[A\n",
            "Generating train examples...:  98%|█████████▊| 65797/67349 [00:14<00:00, 4474.37 examples/s]\u001b[A\n",
            "Generating train examples...:  98%|█████████▊| 66273/67349 [00:14<00:00, 4557.25 examples/s]\u001b[A\n",
            "Generating train examples...:  99%|█████████▉| 66760/67349 [00:14<00:00, 4647.48 examples/s]\u001b[A\n",
            "Generating train examples...: 100%|█████████▉| 67252/67349 [00:14<00:00, 4727.72 examples/s]\u001b[A\n",
            "                                                                                            \u001b[A\n",
            "Shuffling glue-train.tfrecord...:   0%|          | 0/67349 [00:00<?, ? examples/s]\u001b[A\n",
            "Shuffling glue-train.tfrecord...:  15%|█▍        | 9841/67349 [00:00<00:00, 98404.93 examples/s]\u001b[A\n",
            "Shuffling glue-train.tfrecord...:  95%|█████████▌| 64295/67349 [00:00<00:00, 360821.84 examples/s]\u001b[A\n",
            "Generating splits...:  33%|███▎      | 1/3 [00:15<00:30, 15.15s/ splits]\n",
            "Generating validation examples...:   0%|          | 0/872 [00:00<?, ? examples/s]\u001b[A\n",
            "Generating validation examples...:  46%|████▌     | 401/872 [00:00<00:00, 4001.32 examples/s]\u001b[A\n",
            "Generating validation examples...:  92%|█████████▏| 802/872 [00:00<00:00, 3418.65 examples/s]\u001b[A\n",
            "                                                                                             \u001b[A\n",
            "Shuffling glue-validation.tfrecord...:   0%|          | 0/872 [00:00<?, ? examples/s]\u001b[A\n",
            "Generating splits...:  67%|██████▋   | 2/3 [00:15<00:06,  6.40s/ splits]\n",
            "Generating test examples...:   0%|          | 0/1821 [00:00<?, ? examples/s]\u001b[A\n",
            "Generating test examples...:  19%|█▊        | 338/1821 [00:00<00:00, 3371.93 examples/s]\u001b[A\n",
            "Generating test examples...:  40%|████      | 737/1821 [00:00<00:00, 3733.67 examples/s]\u001b[A\n",
            "Generating test examples...:  61%|██████▏   | 1116/1821 [00:00<00:00, 3756.95 examples/s]\u001b[A\n",
            "Generating test examples...:  84%|████████▍ | 1530/1821 [00:00<00:00, 3904.40 examples/s]\u001b[A\n",
            "                                                                                         \u001b[A\n",
            "Shuffling glue-test.tfrecord...:   0%|          | 0/1821 [00:00<?, ? examples/s]\u001b[A\n",
            "\u001b[1mDataset glue downloaded and prepared to /home/ritesh/tensorflow_datasets/glue/sst2/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "FeaturesDict({\n",
            "    'idx': tf.int32,\n",
            "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
            "    'sentence': Text(shape=(), dtype=tf.string),\n",
            "})\n",
            "2\n",
            "['negative', 'positive']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBMPhIdStAe2"
      },
      "source": [
        "# Get the training and validation datasets\n",
        "dataset_train, dataset_validation = dataset['train'], dataset['validation']\n",
        "dataset_train"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbZ-faiNWu1U"
      },
      "source": [
        "# Print some of the entries\n",
        "for example in dataset_train.take(2):  \n",
        "  review, label = example[\"sentence\"], example[\"label\"]\n",
        "  print(\"Review:\", review)\n",
        "  print(\"Label: %d \\n\" % label.numpy())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: tf.Tensor(b'for the uninitiated plays better on video with the sound ', shape=(), dtype=string)\nLabel: 0 \n\nReview: tf.Tensor(b'like a giant commercial for universal studios , where much of the action takes place ', shape=(), dtype=string)\nLabel: 0 \n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fVZItTeZSbL"
      },
      "source": [
        "# Get the sentences and the labels\n",
        "# for both the training and the validation sets\n",
        "training_reviews = []\n",
        "training_labels = []\n",
        " \n",
        "validation_reviews = []\n",
        "validation_labels = []\n",
        "\n",
        "# The dataset has 67,000 training entries, but that's a lot to process here!\n",
        "\n",
        "# If you want to take the entire dataset: WARNING: takes longer!!\n",
        "# for item in dataset_train.take(-1):\n",
        "\n",
        "# Take 10,000 reviews\n",
        "for item in dataset_train.take(10000):\n",
        "  review, label = item[\"sentence\"], item[\"label\"]\n",
        "  training_reviews.append(str(review.numpy()))\n",
        "  training_labels.append(label.numpy())\n",
        "\n",
        "print (\"\\nNumber of training reviews is: \", len(training_reviews))\n",
        "\n",
        "# print some of the reviews and labels\n",
        "for i in range(0, 2):\n",
        "  print (training_reviews[i])\n",
        "  print (training_labels[i])\n",
        "\n",
        "# Get the validation data\n",
        "# there's only about 800 items, so take them all\n",
        "for item in dataset_validation.take(-1):  \n",
        "  review, label = item[\"sentence\"], item[\"label\"]\n",
        "  validation_reviews.append(str(review.numpy()))\n",
        "  validation_labels.append(label.numpy())\n",
        "\n",
        "print (\"\\nNumber of validation reviews is: \", len(validation_reviews))\n",
        "\n",
        "# Print some of the validation reviews and labels\n",
        "for i in range(0, 2):\n",
        "  print (validation_reviews[i])\n",
        "  print (validation_labels[i])\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\nNumber of training reviews is:  10000\nb'for the uninitiated plays better on video with the sound '\n0\nb'like a giant commercial for universal studios , where much of the action takes place '\n0\n\nNumber of validation reviews is:  872\nb'a valueless kiddie paean to pro basketball underwritten by the nba . '\n0\nb\"featuring a dangerously seductive performance from the great daniel auteuil , `` sade '' covers the same period as kaufmann 's `` quills '' with more unsettlingly realistic results . \"\n1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY4ZoptJO55o"
      },
      "source": [
        "# Tokenize the words and sequence the sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TWLvXA1Oa_W"
      },
      "source": [
        "# There's a total of 21224 words in the reviews\n",
        "# but many of them are irrelevant like with, it, of, on.\n",
        "# If we take a subset of the training data, then the vocab\n",
        "# will be smaller.\n",
        "\n",
        "# A reasonable review might have about 50 words or so,\n",
        "# so we can set max_length to 50 (but feel free to change it as you like)\n",
        "\n",
        "vocab_size = 4000\n",
        "embedding_dim = 16\n",
        "max_length = 50\n",
        "trunc_type='post'\n",
        "pad_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_reviews)\n",
        "word_index = tokenizer.word_index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV-Ff5N0ryWv"
      },
      "source": [
        "# Pad the sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-3scEznH2Va"
      },
      "source": [
        "# Pad the sequences so that they are all the same length\n",
        "training_sequences = tokenizer.texts_to_sequences(training_reviews)\n",
        "training_padded = pad_sequences(training_sequences,maxlen=max_length, \n",
        "                                truncating=trunc_type, padding=pad_type)\n",
        "\n",
        "validation_sequences = tokenizer.texts_to_sequences(validation_reviews)\n",
        "validation_padded = pad_sequences(validation_sequences,maxlen=max_length)\n",
        "\n",
        "training_labels_final = np.array(training_labels)\n",
        "validation_labels_final = np.array(validation_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PahZm7YEQ8EI"
      },
      "source": [
        "# Create the model using an Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_nyQeI0RCCv"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),  \n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WRXrx8BRO2L"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBKyVYvxRQ_9"
      },
      "source": [
        "num_epochs = 20\n",
        "history = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n",
        "                    validation_data=(validation_padded, validation_labels_final))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhLPbUl2AZ0y"
      },
      "source": [
        "# Plot the accurracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzBM1PpJAYfD"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEbcMCVEKToB"
      },
      "source": [
        "# Write a function to predict the sentiment of reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0nKY9M4xzWE"
      },
      "source": [
        "# Write some new reviews \n",
        "\n",
        "review1 = \"\"\"I loved this movie\"\"\"\n",
        "\n",
        "review2 = \"\"\"that was the worst movie I've ever seen\"\"\"\n",
        "\n",
        "review3 = \"\"\"too much violence even for a Bond film\"\"\"\n",
        "\n",
        "review4 = \"\"\"a captivating recounting of a cherished myth\"\"\"\n",
        "\n",
        "new_reviews = [review1, review2, review3, review4]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg-maex27KPW"
      },
      "source": [
        "# Define a function to prepare the new reviews for use with a model\n",
        "# and then use the model to predict the sentiment of the new reviews           \n",
        "\n",
        "def predict_review(model, reviews):\n",
        "  # Create the sequences\n",
        "  padding_type='post'\n",
        "  sample_sequences = tokenizer.texts_to_sequences(reviews)\n",
        "  reviews_padded = pad_sequences(sample_sequences, padding=padding_type, \n",
        "                                 maxlen=max_length) \n",
        "  classes = model.predict(reviews_padded)\n",
        "  for x in range(len(reviews_padded)):\n",
        "    print(reviews[x])\n",
        "    print(classes[x])\n",
        "    print('\\n')\n",
        "\n",
        "predict_review(model, new_reviews)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycJKbMq3K4iy"
      },
      "source": [
        "# Define a function to train and show the results of models with different layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PevUcINXK3gn"
      },
      "source": [
        "def fit_model_and_show_results (model, reviews):\n",
        "  model.summary()\n",
        "  history = model.fit(training_padded, training_labels_final, epochs=num_epochs, \n",
        "                      validation_data=(validation_padded, validation_labels_final))\n",
        "  plot_graphs(history, \"accuracy\")\n",
        "  plot_graphs(history, \"loss\")\n",
        "  predict_review(model, reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8jW-OLfTrDM"
      },
      "source": [
        "# Use a CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "merAu9T3TtmQ"
      },
      "source": [
        "num_epochs = 30\n",
        "\n",
        "model_cnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Conv1D(16, 5, activation='relu'),\n",
        "    tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Default learning rate for the Adam optimizer is 0.001\n",
        "# Let's slow down the learning rate by 10.\n",
        "learning_rate = 0.0001\n",
        "model_cnn.compile(loss='binary_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate), \n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "fit_model_and_show_results(model_cnn, new_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXnoq0zITmSM"
      },
      "source": [
        "# Use a GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jP6KAzZTpQ6"
      },
      "source": [
        "num_epochs = 30\n",
        "\n",
        "model_gru = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "learning_rate = 0.00003 # slower than the default learning rate\n",
        "model_gru.compile(loss='binary_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "fit_model_and_show_results(model_gru, new_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U13JBiJUG1oq"
      },
      "source": [
        "# Add a bidirectional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scTUsFPAG4zP"
      },
      "source": [
        "num_epochs = 30\n",
        "\n",
        "model_bidi_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)), \n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "learning_rate = 0.00003\n",
        "model_bidi_lstm.compile(loss='binary_crossentropy',\n",
        "                        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                        metrics=['accuracy'])\n",
        "fit_model_and_show_results(model_bidi_lstm, new_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsxKPbCnPJTj"
      },
      "source": [
        "# Use multiple bidirectional LSTMs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N6Zul47PMED"
      },
      "source": [
        "num_epochs = 30\n",
        "\n",
        "model_multiple_bidi_lstm = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim, \n",
        "                                                       return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "learning_rate = 0.0003\n",
        "model_multiple_bidi_lstm.compile(loss='binary_crossentropy',\n",
        "                                 optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                                 metrics=['accuracy'])\n",
        "fit_model_and_show_results(model_multiple_bidi_lstm, new_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdN2tdW4YYJ1"
      },
      "source": [
        "# Try some more reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e45UQxQl_QAI"
      },
      "source": [
        "# Write some new reviews \n",
        "\n",
        "review1 = \"\"\"I loved this movie\"\"\"\n",
        "\n",
        "review2 = \"\"\"that was the worst movie I've ever seen\"\"\"\n",
        "\n",
        "review3 = \"\"\"too much violence even for a Bond film\"\"\"\n",
        "\n",
        "review4 = \"\"\"a captivating recounting of a cherished myth\"\"\"\n",
        "\n",
        "review5 = \"\"\"I saw this movie yesterday and I was feeling low to start with,\n",
        " but it was such a wonderful movie that it lifted my spirits and brightened \n",
        " my day, you can\\'t go wrong with a movie with Whoopi Goldberg in it.\"\"\"\n",
        "\n",
        "review6 = \"\"\"I don\\'t understand why it received an oscar recommendation\n",
        " for best movie, it was long and boring\"\"\"\n",
        "\n",
        "review7 = \"\"\"the scenery was magnificent, the CGI of the dogs was so realistic I\n",
        " thought they were played by real dogs even though they talked!\"\"\"\n",
        "\n",
        "review8 = \"\"\"The ending was so sad and yet so uplifting at the same time. \n",
        " I'm looking for an excuse to see it again\"\"\"\n",
        "\n",
        "review9 = \"\"\"I had expected so much more from a movie made by the director \n",
        " who made my most favorite movie ever, I was very disappointed in the tedious \n",
        " story\"\"\"\n",
        "\n",
        "review10 = \"I wish I could watch this movie every day for the rest of my life\"\n",
        "\n",
        "more_reviews = [review1, review2, review3, review4, review5, review6, review7, \n",
        "               review8, review9, review10]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ4YZHOjYXQ2"
      },
      "source": [
        "print(\"============================\\n\",\"Embeddings only:\\n\", \"============================\")\n",
        "predict_review(model, more_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI04noTAHK5t"
      },
      "source": [
        "print(\"============================\\n\",\"With CNN\\n\", \"============================\")\n",
        "predict_review(model_cnn, more_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGJ32sRUHNu6"
      },
      "source": [
        "print(\"===========================\\n\",\"With bidirectional GRU\\n\", \"============================\")\n",
        "predict_review(model_gru, more_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFw9Q0iQHP2P"
      },
      "source": [
        "print(\"===========================\\n\", \"With a single bidirectional LSTM:\\n\", \"===========================\")\n",
        "predict_review(model_bidi_lstm, more_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaw7fEVQHSWK"
      },
      "source": [
        "print(\"===========================\\n\", \"With multiple bidirectional LSTM:\\n\", \"==========================\")\n",
        "predict_review(model_multiple_bidi_lstm, more_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}