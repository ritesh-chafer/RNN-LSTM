{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l10c04_nlp_optimizing_the_text_generation_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit"
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Optimizing the Text Generation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCxhW3mtLmfb"
      },
      "source": [
        "You've already done some amazing work with generating new songs, but so far we've seen some issues with repetition and a fair amount of incoherence. By using more data and further tweaking the model, you'll be able to get improved results. We'll once again use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics) here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bf5FVHfganK"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    \"https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\" \\\n",
        "    -O /tmp/songdata.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-06-29 20:23:45--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.27.206, 2404:6800:4009:800::200e\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.27.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/h41hk4mosf0u2tlo41hsq9odt2mnqoij/1624978425000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-06-29 20:23:48--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/h41hk4mosf0u2tlo41hsq9odt2mnqoij/1624978425000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 142.250.192.65, 2404:6800:4009:829::2001\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|142.250.192.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv       [   <=>              ]  69.08M  3.15MB/s    in 22s     \n",
            "\n",
            "2021-06-29 20:24:11 (3.11 MB/s) - ‘/tmp/songdata.csv’ saved [72436445]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz9x-7dWihxx"
      },
      "source": [
        "## 250 Songs\n",
        "\n",
        "Now we've seen a model trained on just a small sample of songs, and how this often leads to repetition as you get further along in trying to generate new text. Let's switch to using the 250 songs instead, and see if our output improves. This will actually be nearly 10K lines of lyrics, which should be sufficient.\n",
        "\n",
        "Note that we won't use the full dataset here as it will take up quite a bit of RAM and processing time, but you're welcome to try doing so on your own later. If interested, you'll likely want to use only some of the more common words for the Tokenizer, which will help shrink processing time and memory needed (or else you'd have an output array hundreds of thousands of words long)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWbMN_19jfRT"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRmPPJegovBe"
      },
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIGedF3XjHj4"
      },
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "# Read the dataset from csv - this time with 250 songs\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:250]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus, num_words=2000)\n",
        "total_words = tokenizer.num_words\n",
        "\n",
        "# There should be a lot more words now\n",
        "print(total_words)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n",
            "<ipython-input-3-fbdddccf8583>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quoDmw_FkNBA"
      },
      "source": [
        "### Create Sequences and Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkLAf3HmkPSo"
      },
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cECbqT-blMk-"
      },
      "source": [
        "### Train a (Better) Text Generation Model\n",
        "\n",
        "With more data, we'll cut off after 100 epochs to avoid keeping you here all day. You'll also want to change your runtime type to GPU if you haven't already (you'll need to re-run the above cells if you change runtimes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nHOp6uWlP_P"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=100, verbose=1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1480/1480 [==============================] - 16s 10ms/step - loss: 5.9820 - accuracy: 0.0460\n",
            "Epoch 2/100\n",
            "1480/1480 [==============================] - 14s 10ms/step - loss: 5.6908 - accuracy: 0.0513\n",
            "Epoch 3/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 5.4908 - accuracy: 0.0683\n",
            "Epoch 4/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 5.2931 - accuracy: 0.0954\n",
            "Epoch 5/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 5.0899 - accuracy: 0.1166\n",
            "Epoch 6/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 4.8948 - accuracy: 0.1354\n",
            "Epoch 7/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 4.7293 - accuracy: 0.1521\n",
            "Epoch 8/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 4.5899 - accuracy: 0.1671\n",
            "Epoch 9/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 4.4685 - accuracy: 0.1805\n",
            "Epoch 10/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 4.3599 - accuracy: 0.1944\n",
            "Epoch 11/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 4.2607 - accuracy: 0.2055\n",
            "Epoch 12/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 4.1650 - accuracy: 0.2160\n",
            "Epoch 13/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 4.0811 - accuracy: 0.2238\n",
            "Epoch 14/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 4.0001 - accuracy: 0.2357\n",
            "Epoch 15/100\n",
            "1480/1480 [==============================] - 14s 10ms/step - loss: 3.9289 - accuracy: 0.2453\n",
            "Epoch 16/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 3.8599 - accuracy: 0.2534\n",
            "Epoch 17/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 3.8007 - accuracy: 0.2627\n",
            "Epoch 18/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 3.7416 - accuracy: 0.2709\n",
            "Epoch 19/100\n",
            "1480/1480 [==============================] - 17s 11ms/step - loss: 3.6846 - accuracy: 0.2791\n",
            "Epoch 20/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 3.6355 - accuracy: 0.2874\n",
            "Epoch 21/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 3.5888 - accuracy: 0.2910\n",
            "Epoch 22/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 3.5415 - accuracy: 0.2997\n",
            "Epoch 23/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 3.5102 - accuracy: 0.3024\n",
            "Epoch 24/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 3.4627 - accuracy: 0.3085\n",
            "Epoch 25/100\n",
            "1480/1480 [==============================] - 15s 10ms/step - loss: 3.4240 - accuracy: 0.3140\n",
            "Epoch 26/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 3.3842 - accuracy: 0.3200\n",
            "Epoch 27/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 3.3498 - accuracy: 0.3280\n",
            "Epoch 28/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 3.3229 - accuracy: 0.3327\n",
            "Epoch 29/100\n",
            "1480/1480 [==============================] - 28s 19ms/step - loss: 3.2824 - accuracy: 0.3377\n",
            "Epoch 30/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.2583 - accuracy: 0.3415\n",
            "Epoch 31/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.2244 - accuracy: 0.3445\n",
            "Epoch 32/100\n",
            "1480/1480 [==============================] - 17s 12ms/step - loss: 3.1965 - accuracy: 0.3485\n",
            "Epoch 33/100\n",
            "1480/1480 [==============================] - 17s 11ms/step - loss: 3.1666 - accuracy: 0.3532\n",
            "Epoch 34/100\n",
            "1480/1480 [==============================] - 17s 11ms/step - loss: 3.1411 - accuracy: 0.3584\n",
            "Epoch 35/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.1146 - accuracy: 0.3614\n",
            "Epoch 36/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 3.0945 - accuracy: 0.3657\n",
            "Epoch 37/100\n",
            "1480/1480 [==============================] - 17s 12ms/step - loss: 3.0725 - accuracy: 0.3667\n",
            "Epoch 38/100\n",
            "1480/1480 [==============================] - 17s 12ms/step - loss: 3.0417 - accuracy: 0.3739\n",
            "Epoch 39/100\n",
            "1480/1480 [==============================] - 21s 14ms/step - loss: 3.0242 - accuracy: 0.3767\n",
            "Epoch 40/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 3.0027 - accuracy: 0.3798\n",
            "Epoch 41/100\n",
            "1480/1480 [==============================] - 17s 12ms/step - loss: 2.9828 - accuracy: 0.3839\n",
            "Epoch 42/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 2.9614 - accuracy: 0.3881\n",
            "Epoch 43/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 2.9382 - accuracy: 0.3905\n",
            "Epoch 44/100\n",
            "1480/1480 [==============================] - 17s 12ms/step - loss: 2.9256 - accuracy: 0.3922\n",
            "Epoch 45/100\n",
            "1480/1480 [==============================] - 20s 13ms/step - loss: 2.9087 - accuracy: 0.3962\n",
            "Epoch 46/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.8851 - accuracy: 0.4005\n",
            "Epoch 47/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.8690 - accuracy: 0.4034\n",
            "Epoch 48/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.8471 - accuracy: 0.4064\n",
            "Epoch 49/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.8512 - accuracy: 0.4054\n",
            "Epoch 50/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.8204 - accuracy: 0.4121\n",
            "Epoch 51/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.8049 - accuracy: 0.4130\n",
            "Epoch 52/100\n",
            "1480/1480 [==============================] - 17s 11ms/step - loss: 2.7869 - accuracy: 0.4177\n",
            "Epoch 53/100\n",
            "1480/1480 [==============================] - 17s 11ms/step - loss: 2.7777 - accuracy: 0.4198\n",
            "Epoch 54/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.7646 - accuracy: 0.4206\n",
            "Epoch 55/100\n",
            "1480/1480 [==============================] - 17s 11ms/step - loss: 2.7409 - accuracy: 0.4261\n",
            "Epoch 56/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 2.7317 - accuracy: 0.4254\n",
            "Epoch 57/100\n",
            "1480/1480 [==============================] - 17s 12ms/step - loss: 2.7216 - accuracy: 0.4270\n",
            "Epoch 58/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.7037 - accuracy: 0.4318\n",
            "Epoch 59/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.6968 - accuracy: 0.4314\n",
            "Epoch 60/100\n",
            "1480/1480 [==============================] - 17s 11ms/step - loss: 2.6837 - accuracy: 0.4341\n",
            "Epoch 61/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.6714 - accuracy: 0.4380\n",
            "Epoch 62/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.6575 - accuracy: 0.4392\n",
            "Epoch 63/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.6363 - accuracy: 0.4434\n",
            "Epoch 64/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.6302 - accuracy: 0.4431\n",
            "Epoch 65/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.6205 - accuracy: 0.4457\n",
            "Epoch 66/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.6215 - accuracy: 0.4441\n",
            "Epoch 67/100\n",
            "1480/1480 [==============================] - 23s 16ms/step - loss: 2.5967 - accuracy: 0.4517\n",
            "Epoch 68/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.5828 - accuracy: 0.4537\n",
            "Epoch 69/100\n",
            "1480/1480 [==============================] - 22s 15ms/step - loss: 2.5746 - accuracy: 0.4532\n",
            "Epoch 70/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 2.5676 - accuracy: 0.4539\n",
            "Epoch 71/100\n",
            "1480/1480 [==============================] - 17s 12ms/step - loss: 2.5498 - accuracy: 0.4567\n",
            "Epoch 72/100\n",
            "1480/1480 [==============================] - 17s 11ms/step - loss: 2.5462 - accuracy: 0.4582\n",
            "Epoch 73/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.5325 - accuracy: 0.4622\n",
            "Epoch 74/100\n",
            "1480/1480 [==============================] - 17s 12ms/step - loss: 2.5308 - accuracy: 0.4608\n",
            "Epoch 75/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 2.5172 - accuracy: 0.4638\n",
            "Epoch 76/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.5093 - accuracy: 0.4643\n",
            "Epoch 77/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.4976 - accuracy: 0.4664\n",
            "Epoch 78/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.4969 - accuracy: 0.4663\n",
            "Epoch 79/100\n",
            "1480/1480 [==============================] - 21s 14ms/step - loss: 2.4767 - accuracy: 0.4715\n",
            "Epoch 80/100\n",
            "1480/1480 [==============================] - 17s 12ms/step - loss: 2.4720 - accuracy: 0.4711\n",
            "Epoch 81/100\n",
            "1480/1480 [==============================] - 17s 11ms/step - loss: 2.4715 - accuracy: 0.4694\n",
            "Epoch 82/100\n",
            "1480/1480 [==============================] - 17s 11ms/step - loss: 2.4528 - accuracy: 0.4743\n",
            "Epoch 83/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.4484 - accuracy: 0.4747\n",
            "Epoch 84/100\n",
            "1480/1480 [==============================] - 17s 11ms/step - loss: 2.4408 - accuracy: 0.4773\n",
            "Epoch 85/100\n",
            "1480/1480 [==============================] - 17s 12ms/step - loss: 2.4303 - accuracy: 0.4787\n",
            "Epoch 86/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.4403 - accuracy: 0.4763\n",
            "Epoch 87/100\n",
            "1480/1480 [==============================] - 16s 11ms/step - loss: 2.4542 - accuracy: 0.4748\n",
            "Epoch 88/100\n",
            "1480/1480 [==============================] - 17s 11ms/step - loss: 2.4131 - accuracy: 0.4840\n",
            "Epoch 89/100\n",
            "1480/1480 [==============================] - 17s 12ms/step - loss: 2.4043 - accuracy: 0.4835\n",
            "Epoch 90/100\n",
            "1480/1480 [==============================] - 19s 13ms/step - loss: 2.3974 - accuracy: 0.4860\n",
            "Epoch 91/100\n",
            "1480/1480 [==============================] - 30s 20ms/step - loss: 2.3902 - accuracy: 0.4870\n",
            "Epoch 92/100\n",
            "1480/1480 [==============================] - 21s 14ms/step - loss: 2.3941 - accuracy: 0.4857\n",
            "Epoch 93/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 2.3740 - accuracy: 0.4887\n",
            "Epoch 94/100\n",
            "1480/1480 [==============================] - 17s 12ms/step - loss: 2.3643 - accuracy: 0.4917\n",
            "Epoch 95/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 2.3656 - accuracy: 0.4903\n",
            "Epoch 96/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 2.3547 - accuracy: 0.4924\n",
            "Epoch 97/100\n",
            "1480/1480 [==============================] - 18s 12ms/step - loss: 2.3555 - accuracy: 0.4917\n",
            "Epoch 98/100\n",
            "1480/1480 [==============================] - 22s 15ms/step - loss: 2.3358 - accuracy: 0.4971\n",
            "Epoch 99/100\n",
            "1480/1480 [==============================] - 28s 19ms/step - loss: 2.3326 - accuracy: 0.4973\n",
            "Epoch 100/100\n",
            "1480/1480 [==============================] - 25s 17ms/step - loss: 2.3226 - accuracy: 0.4986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgvIz20nlQcq"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOqmmarvlSLh"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISLZZGlQlSxh"
      },
      "source": [
        "### Generate better lyrics!\n",
        "\n",
        "This time around, we should be able to get a more interesting output with less repetition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P96oVMk3lU7y"
      },
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgJKV8_oRU9"
      },
      "source": [
        "### Varying the Possible Outputs\n",
        "\n",
        "In running the above, you may notice that the same seed text will generate similar outputs. This is because the code is currently always choosing the top predicted class as the next word. What if you wanted more variance in the output? \n",
        "\n",
        "Switching from `model.predict_classes` to `model.predict_proba` will get us all of the class probabilities. We can combine this with `np.random.choice` to select a given predicted output based on a probability, thereby giving a bit more randomness to our outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZe9gaJeoGVP"
      },
      "source": [
        "# Test the method with just the first word after the seed text\n",
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "predicted_probs = model.predict(token_list)[0]\n",
        "predicted = np.random.choice([x for x in range(len(predicted_probs))], \n",
        "                             p=predicted_probs)\n",
        "# Running this cell multiple times should get you some variance in output\n",
        "print(predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee7WKgRGrJy1"
      },
      "source": [
        "# Use this process for the full output generation\n",
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "  predicted_probs = model.predict(token_list)[0]\n",
        "  predicted = np.random.choice([x for x in range(len(predicted_probs))],\n",
        "                               p=predicted_probs)\n",
        "  output_word = \"\"\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted:\n",
        "      output_word = word\n",
        "      break\n",
        "  seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}